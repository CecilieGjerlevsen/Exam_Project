---
title: "Loan defaults"
date: "12/06/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

Loading the two datasets
```{r, echo=FALSE}
application_data <- read.csv("application_data.csv")
descriptions <- read.csv("columns_description.csv")
```

Loading packages:
```{r, echo=FALSE}
pacman::p_load(
  ggplot2, #for plotting graphics
  data.table, #for working with data.table
  dplyr, #for count function
  tidyverse,
  skimr,
  psych,
  ggcorrplot,
  scales
)
```

#Data profiling
## Single-column profiling
```{r, warning = FALSE, results = 'hide'}
#Using options() to ensure that numbers are not displayed in scientific notation
options(scipen = 999)

#Number of rows and columns
dim(application_data)

#Calculating max, min, mean, range, standard deviation and standard error of each of the variables
cardinalities <- as.data.table(describe(application_data),keep.rownames=TRUE)

#Adding the datatype of each variable
cardinalities$class <- sapply(application_data,class)

#Adding the number of unique values for each variable
cardinalities$distinct <- sapply(application_data, n_distinct)

#Calculating the number of missing values (NA) for each variable, where we first replace all blanks "" with NA
application_data[application_data == ""] <- NA
cardinalities$missing <- colSums(is.na(application_data))
```

Plot of percentage missing values
```{r}
#Calculating and storing the number of observations
n <- nrow(application_data) 

#Calculating the percentage missing for each variable
cardinalities$pct_missing <- cardinalities$missing/n * 100

#Modifying the independent variable to be a factor
cardinalities$rn <- factor(cardinalities$rn, levels = cardinalities$rn)

cardinalities %>% 
  ggplot(aes(rn,pct_missing)) +
  geom_segment( aes(x=rn, xend=rn, y=0, yend=pct_missing)) +
  geom_point(color = "blue") +
  geom_hline(yintercept=40, linetype="dashed", color = "red") +
  labs(x = "", y = "Percentage missing (%)") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(limits=cardinalities$rn) 
```
Removing columns with NA values over a limit of 40%
```{r}
#Defining a new dataset for data cleansing
clean_data <- application_data

# Counting the number of variables with more than 40% missing values.
limit <- 0.4 * n #setting a limit for which columns to remove, where the limit is set to be 40%

#Creating a column with TRUE and FALSE values depending on whether the column should be deleted or not
cardinalities$delete <- cardinalities$missing > limit 

#Deleting the 49 variables with more than 40% missing values 
clean_data <- clean_data[, which(colMeans(is.na(clean_data)) < 0.4)] 
```

The variables called FLAG_Document_# (from 2-21) all have different means indicating that there is a difference in how many documents each applicant have handed in. However, we do not have any information about the content of each document. To use the information in these variables in more manageable way, we aggregate them to one new variables, which counts the number of documents each applicant has submitted.
```{r}
clean_data$DOCUMENTS_PROVIDED <- clean_data$FLAG_DOCUMENT_2 + clean_data$FLAG_DOCUMENT_3 + clean_data$FLAG_DOCUMENT_4 + clean_data$FLAG_DOCUMENT_5 + clean_data$FLAG_DOCUMENT_6 + clean_data$FLAG_DOCUMENT_7 + clean_data$FLAG_DOCUMENT_8 + clean_data$FLAG_DOCUMENT_9 + clean_data$FLAG_DOCUMENT_10 + clean_data$FLAG_DOCUMENT_11 + clean_data$FLAG_DOCUMENT_12 + clean_data$FLAG_DOCUMENT_13 + clean_data$FLAG_DOCUMENT_14 + clean_data$FLAG_DOCUMENT_15 + clean_data$FLAG_DOCUMENT_16 + clean_data$FLAG_DOCUMENT_17 + clean_data$FLAG_DOCUMENT_18 + clean_data$FLAG_DOCUMENT_19 + clean_data$FLAG_DOCUMENT_20 + clean_data$FLAG_DOCUMENT_21

#Then, we remove the individual document variables
clean_data <- clean_data[,!(names(clean_data) %in% c("FLAG_DOCUMENT_2","FLAG_DOCUMENT_3","FLAG_DOCUMENT_4","FLAG_DOCUMENT_5","FLAG_DOCUMENT_6","FLAG_DOCUMENT_7","FLAG_DOCUMENT_8","FLAG_DOCUMENT_9","FLAG_DOCUMENT_10","FLAG_DOCUMENT_11","FLAG_DOCUMENT_12","FLAG_DOCUMENT_13","FLAG_DOCUMENT_14","FLAG_DOCUMENT_15","FLAG_DOCUMENT_16","FLAG_DOCUMENT_17","FLAG_DOCUMENT_18","FLAG_DOCUMENT_19","FLAG_DOCUMENT_20","FLAG_DOCUMENT_21"))]
```

We combine the AMT_REQ_CREDIT (number of enquiries to credit bureau about the client) variables into one variable, ENQUIRIES. This is possible since the variables exclude each other. For example, the WEEK variable excludes one day before application.
```{r}
clean_data$ENQUIRIES <- clean_data$AMT_REQ_CREDIT_BUREAU_HOUR + clean_data$AMT_REQ_CREDIT_BUREAU_DAY + clean_data$AMT_REQ_CREDIT_BUREAU_WEEK + clean_data$AMT_REQ_CREDIT_BUREAU_MON + clean_data$AMT_REQ_CREDIT_BUREAU_QRT + clean_data$AMT_REQ_CREDIT_BUREAU_YEAR

#Then, we remove the individual document variables
clean_data <- clean_data[,!(names(clean_data) %in% c("AMT_REQ_CREDIT_BUREAU_HOUR","AMT_REQ_CREDIT_BUREAU_DAY","AMT_REQ_CREDIT_BUREAU_WEEK","AMT_REQ_CREDIT_BUREAU_MON","AMT_REQ_CREDIT_BUREAU_QRT","AMT_REQ_CREDIT_BUREAU_YEAR"))]
```

Changing the variables from days to years to make them interpretable
```{r}
clean_data$AGE <- floor(abs(clean_data$DAYS_BIRTH / 365))
clean_data$YEARS_EMPLOYED <- abs(clean_data$DAYS_EMPLOYED / 365)
clean_data$YEARS_REGISTRATION <- abs(clean_data$DAYS_REGISTRATION / 365)
clean_data$YEARS_PUBLISH <- abs(clean_data$DAYS_ID_PUBLISH / 365)
clean_data$YEARS_PHONE_CHANGE <- abs(clean_data$DAYS_LAST_PHONE_CHANGE / 365)

#Removing the old variables
clean_data <- clean_data[,!(names(clean_data) %in% c("DAYS_BIRTH","DAYS_EMPLOYED","DAYS_REGISTRATION","DAYS_ID_PUBLISH","DAYS_LAST_PHONE_CHANGE"))]
```

## Multi-column profiling
The scope of this multi-column profiling is to remove unnecessary columns. 

We start by going through all possible feature variables which are categorical or binary. We focus on the variables where we suspect overlaps.

Starting with the variables: Did the client provide XX? 

```{r}
for(i in 1:6) 
 print(ggplot(clean_data, aes(x = TARGET, fill = factor(clean_data[,c(17+i)]))) +
 geom_bar(position = "dodge") +
 labs(y = "Count", x = "Target variable", fill = "Did client provide XX?") + 
 scale_x_continuous(breaks=c(0,1)) + 
 scale_fill_discrete(labels = c("No", "Yes")))
```

We remove the variables "FLAG_MOBIL" "FLAG_WORK_PHONE" and "FLAG_CONT_MOBILE":
- Did client provide mobile phone (1=YES, 0=NO) = TO DROP (there is nobody who has answered no)
- Did client provide work phone (1=YES, 0=NO) = TO KEEP
- Did client provide home phone (1=YES, 0=NO) = TO DROP (two variables which indicate home phone, we remove this one because there is a discrepancy between name of variable and description of variable)
- Was mobile phone reachable (1=YES, 0=NO) = TO DROP (there is approximately zero who has answered no)
- Did client provide home phone (1=YES, 0=NO) = TO KEEP
- Did client provide email (1=YES, 0=NO) = TO KEEP
```{r}
clean_data <- clean_data[,!(names(clean_data) %in% c("FLAG_MOBIL", "FLAG_WORK_PHONE", "FLAG_CONT_MOBILE"))]
```

We check the variables regarding if different addresses provided match.
```{r}
model.matrix(~0+., clean_data[,c("REG_REGION_NOT_LIVE_REGION","REG_REGION_NOT_WORK_REGION","LIVE_REGION_NOT_WORK_REGION","REG_CITY_NOT_LIVE_CITY","REG_CITY_NOT_WORK_CITY","LIVE_CITY_NOT_WORK_CITY","TARGET")]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
We remove the 3 variables which are related to region, since if it matches at the city level it is expected to also match at region level.
```{r}
clean_data <- clean_data[,!(names(clean_data) %in% c("REG_REGION_NOT_LIVE_REGION","REG_REGION_NOT_WORK_REGION","LIVE_REGION_NOT_WORK_REGION"))]
```

Furthermore, we notice the variables about social surroundings, which we expect to have high correlations:
```{r}
model.matrix(~0+., clean_data[,c("OBS_30_CNT_SOCIAL_CIRCLE","DEF_30_CNT_SOCIAL_CIRCLE","OBS_60_CNT_SOCIAL_CIRCLE","DEF_60_CNT_SOCIAL_CIRCLE","TARGET")]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```
We remove the 30-day variables since the 60 day variables all else equal must contain more information:
```{r}
clean_data <- clean_data[,!(names(clean_data) %in% c("OBS_30_CNT_SOCIAL_CIRCLE","DEF_30_CNT_SOCIAL_CIRCLE"))]
```

Due to missing descriptions/anonymousness of the variables containing external source data, we delete these two.
```{r}
clean_data <- clean_data[,!(names(clean_data) %in% c("EXT_SOURCE_2","EXT_SOURCE_3"))]
```

#Outlier analysis
Making a new summary statistics datatable with the remaining variables.
```{r, warning = FALSE}
cardinalities_clean <- as.data.table(describe(clean_data),keep.rownames=TRUE)
cardinalities_clean$class <- sapply(clean_data,class)
cardinalities_clean$distinct <- sapply(clean_data, n_distinct)
cardinalities_clean$missing <- colSums(is.na(clean_data))
```

We look at the categorical values, the number of distinct answers and the frequency. If there are few outliers, the most frequent answer is imputed. However, if a large share of the dataset is unknown, a new category called "Unknown" is created for these observations. Nothing is done to the variables where there seems to be no irregularities.
```{r}
#Gender
count(clean_data, CODE_GENDER) #We observe that 'CODE_Gender' has 3 distinct values, where one of them is 'XNA' 
clean_data$CODE_GENDER <- replace(clean_data$CODE_GENDER,clean_data$CODE_GENDER == "XNA","F") #We replace XNA with female

#Organisation type
count(clean_data, ORGANIZATION_TYPE)
clean_data$ORGANIZATION_TYPE <- replace(clean_data$ORGANIZATION_TYPE,clean_data$ORGANIZATION_TYPE == "XNA","Unknown") #We replace XNA with unknown
```

We look at the all numerical variables to spot irregularities and correct for them when detected. 
```{r}
#Total income
ggplot(clean_data,aes(AMT_INCOME_TOTAL)) + 
  geom_dots()

clean_data$AMT_INCOME_TOTAL <- replace(clean_data$AMT_INCOME_TOTAL,clean_data$AMT_INCOME_TOTAL > 100000000, 
                                       mean(clean_data$AMT_INCOME_TOTAL)) ##Imputing the extreme value above 100.000.000 in income with the mean

#Number of children
clean_data %>% 
  ggplot(aes(CNT_CHILDREN)) +
  geom_bar() 

##Since there are no observations between 14 and 19 children, the two with 19 children could be outliers.We therefore compare the variable to the number of family members (CNT_FAM_MEMBERS)
clean_data %>% 
  ggplot(aes(CNT_FAM_MEMBERS)) +
  geom_bar()

sum(clean_data$CNT_CHILDREN > clean_data$CNT_FAM_MEMBERS) #Comparing the CNT_FAM_MEMBERS and CNT_CHILDREN variables, we can see that all CNT_CHILDREN variables are smaller than CNT_FAM_MEMBERS. This indicates that the high numbers of children is not outliers.

#OBS_60_CNT_SOCIAL_CIRCLE
##The variable OBS_60_CNT_SOCIAL_CIRCLE has a high maximum. We inspect the variable further.
table(clean_data$OBS_60_CNT_SOCIAL_CIRCLE)

clean_data$OBS_60_CNT_SOCIAL_CIRCLE <- replace(clean_data$OBS_60_CNT_SOCIAL_CIRCLE,clean_data$OBS_60_CNT_SOCIAL_CIRCLE > 300, round(mean(clean_data$OBS_60_CNT_SOCIAL_CIRCLE))) #The high observation of 344 looks like an outlier, so it is imputed by the rounded mean

#Years employed
##Years employed has a maximum of 1000.6657534
length(which(clean_data$YEARS_EMPLOYED > 1000)) #There are 55374 instances of this outcome

clean_data %>% 
  ggplot(aes(YEARS_EMPLOYED,AGE)) +
  geom_point() #The distribution suggests the number is a data entry error and is therefore treated as missing values. 
clean_data <- clean_data[,!(names(clean_data) %in% c("YEARS_EMPLOYED"))] #We delete the variable since imputation would be expected to skew the results
```

#Imputation
Impute missing values in clean_data.

We start with imputing the categorical variables as follows:
- For categorical variables with low null percentage, impute with the most frequent items.
- For categorical variables with high null percentage, create new category as not to influence the analysis.
```{r}
#Creating a function that "calculates" the most frequent observation of a categorical value
calculate_mode <- function(x) {
  uniqx <- unique(na.omit(x))
  uniqx[which.max(tabulate(match(x, uniqx)))]}

#NAME_TYPE_SUITE
clean_data["NAME_TYPE_SUITE"] <- replace(clean_data["NAME_TYPE_SUITE"],is.na(clean_data["NAME_TYPE_SUITE"]), calculate_mode(clean_data$NAME_TYPE_SUITE))

#Occupation type
clean_data["OCCUPATION_TYPE"] <- replace(clean_data["OCCUPATION_TYPE"],is.na(clean_data["OCCUPATION_TYPE"]), "Unknown")
```

We impute the integer and numerical variables.
```{r}
#Starting with all integer variables and imputing the median
clean_data[,c("CNT_FAM_MEMBERS")][is.na(clean_data[,c("CNT_FAM_MEMBERS")])] <- median(clean_data[,c("CNT_FAM_MEMBERS")], na.rm = TRUE)

clean_data[,c("DEF_60_CNT_SOCIAL_CIRCLE")][is.na(clean_data[,c("DEF_60_CNT_SOCIAL_CIRCLE")])] <- median(clean_data[,c("DEF_60_CNT_SOCIAL_CIRCLE")], na.rm = TRUE)

clean_data[,c("OBS_60_CNT_SOCIAL_CIRCLE")][is.na(clean_data[,c("OBS_60_CNT_SOCIAL_CIRCLE")])] <- median(clean_data[,c("OBS_60_CNT_SOCIAL_CIRCLE")], na.rm = TRUE)

clean_data[,c("ENQUIRIES")][is.na(clean_data[,c("ENQUIRIES")])] <- median(clean_data[,c("ENQUIRIES")], na.rm = TRUE)

#Imputing the remaning numerical variables with the mean of the variable
for(i in 1:ncol(clean_data))
  clean_data[ , i][is.na(clean_data[ , i])] <- mean(clean_data[ , i], na.rm=TRUE)
```

# Data preparation
FLAG own car & FLAG own realty are turned into 0, 1 answers:
```{r}
clean_data$FLAG_OWN_CAR[clean_data$FLAG_OWN_CAR == "Y"] <- 1
clean_data$FLAG_OWN_CAR[clean_data$FLAG_OWN_CAR == "N"] <- 0

clean_data$FLAG_OWN_REALTY[clean_data$FLAG_OWN_REALTY == "Y"] <- 1
clean_data$FLAG_OWN_REALTY[clean_data$FLAG_OWN_REALTY == "N"] <- 0
```

Looking at who accompanied the client applying for the loan:
```{r}
ggplot(clean_data,aes(x = NAME_TYPE_SUITE, fill = factor(TARGET,labels = c("No", "Yes")))) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  theme_half_open(12) +
  theme(legend.justification = "top") +
  xlab("Accompanying party") +
  ylab("Percentage") +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_fill_discrete(name = "Payment difficulties") +
  theme(legend.key.size = unit(1, 'cm')) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
We aggregate "NAME_TYPE_SUITE" into Accompanied versus Unaccompanied:
```{r}
clean_data$NAME_TYPE_SUITE[clean_data$NAME_TYPE_SUITE == "Children"] <- "Accompanied"
clean_data$NAME_TYPE_SUITE[clean_data$NAME_TYPE_SUITE == "Family"] <- "Accompanied"
clean_data$NAME_TYPE_SUITE[clean_data$NAME_TYPE_SUITE == "Group of people"] <- "Accompanied"
clean_data$NAME_TYPE_SUITE[clean_data$NAME_TYPE_SUITE == "Other_A"] <- "Accompanied"
clean_data$NAME_TYPE_SUITE[clean_data$NAME_TYPE_SUITE == "Other_B"] <- "Accompanied"
clean_data$NAME_TYPE_SUITE[clean_data$NAME_TYPE_SUITE == "Spouse, partner"] <- "Accompanied"
```

Collecting variables within organisation type:
```{r}
#Business entities
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Business Entity Type 1"] <- "Business Entity"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Business Entity Type 2"] <- "Business Entity"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Business Entity Type 3"] <- "Business Entity"

#Industry
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 1"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 2"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 3"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 4"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 5"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 6"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 7"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 8"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 9"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 10"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 11"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 12"] <- "Industry"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Industry: type 13"] <- "Industry"

#Trade
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 1"] <- "Trade"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 2"] <- "Trade"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 3"] <- "Trade"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 4"] <- "Trade"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 5"] <- "Trade"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 6"] <- "Trade"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Trade: type 7"] <- "Trade"

#Transport
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Transport: type 1"] <- "Transport"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Transport: type 2"] <- "Transport"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Transport: type 3"] <- "Transport"
clean_data$ORGANIZATION_TYPE[clean_data$ORGANIZATION_TYPE == "Transport: type 4"] <- "Transport"
```

```{r}
master_data <- clean_data
write.csv(master_data,"master_data.csv")
```


## EXPERIMENT: Principal Coponent Analysis (PCA)
Data scientists use PCA analysis when they face a large set of correlated variables. This method allows us to summarize these correlated variables into principal components (a smaller number of representative variables). 

Thus before we try to do PCA. Lets first see if we have large set of correlated variables in or data set (master_data).


```{r}
str(master_data)
```
### Test for colinearity ap p=0.05:

```{r}
# Tis code shows us the correlation matrix in a nice table
#install.packages("pander")
library(pander)
correlation_imputed<-pander(round(cor(master_data[,unlist(lapply(master_data, is.numeric))]),2))
correlation_imputed

#Use corr_cross() function to compute all correlations and return the highest and significant ones in a plot
#install.packages("lares")
library(lares)

corr_cross(master_data[,unlist(lapply(master_data, is.numeric))],#dataset (only numerical)
  max_pvalue = 0.05, # display only significant correlations (at 5% level)
  top = 10 # display top 10 couples of variables (by correlation coefficient)
)

# the plot of ranked cross-correlation tells us that there is correlation between variables which will lead to the multidisciplinary problem. Thus it is possible to use PCA to resolve the multicollinearity problem through dimension reduction.

```


1. The first objective of the PCA is to evaluate whether variables can be summarized, in order to see a bigger picture in terms of understanding what increases the probability of loan default. The outcome of the the PCA are principal components that contain an information about the interrelationships between variables. PCA will result in a smaller set of concepts to consider in evaluation whether to provide the loan or not. Each principal component will contain specific variables that are a facet of the broader evaluative dimension. 
  - Assess which variables are most responsible for similarities or differences between the clients
  - Assess which clients are similar to one another, and which ones are different.
  
2. The second objective of the PCA is to reduce the 38 variables, that we have ended up after the data cleaning, to a smaller number of principal components to be used in a supervised analysis.


```{r}
# Here we set up the margins of the text in LAText document; might need to install formatR library
library(knitr) 
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE)
```

### We select relevant variables:
PCA should only contain relevant continuous variables, and we can apply PCA to only those variables that have the same scale (e.g. all variables are stated in dollars).
```{r}
# Next we have to Select variables aimed for PCA. It is important for make sure these variables are all metric.
imputed_PCA <- master_data[,c(3:38)] # here we have removed SK_ID_CURR (ID variable), and TARGET (target, dependent variable)
imputed_PCA <- imputed_PCA[,unlist(lapply(imputed_PCA, is.numeric))] # Here we have selected only numerical variables, all character variables were removed

# For the PCA we have selected 24 variables

str(imputed_PCA)
```

We can apply PCA only to the continuous variables that have the same scale (e.g. all variables are stated in dollars). Our dataset does not have enough variables of the same scale. Thus, we cannot use PCA.







