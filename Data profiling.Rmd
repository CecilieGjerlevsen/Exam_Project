---
title: "Loan defaults"
date: "12/06/2021"
output: html_document
---

Loading the three datasets
```{r, echo=FALSE}
#Loading the dataset
application_data <- read.csv("application_data.csv")
descriptions <- read.csv("columns_description.csv")
previous_applications <- read.csv("previous_application.csv")
```

Loading packages:
```{r, echo=FALSE}
pacman::p_load(
  ggplot2, #for plotting graphics
  ggmap, #for plotting maps
  RColorBrewer, #provide pretty colour palettes
  lubridate, #for converting time variables into date format
  hrbrthemes, #for more style themes
  data.table, #for working with data.table
  dplyr, #for count function
  tidytext, #for text mining functions such as unnest()
  wordcloud,
  tm, #for removing stopwords
  ggdist, #for raincloud plots
  tidyquant, #for theme_tq
  prismatic, #for color handling in plots
  gganimate, #for animated plots
  gifski, #for rendering animated plots as GIFs instead of single frames
  tidyverse,
  skimr
)
```

##Data profiling
# Cardinalities (single-column analysis)
```{r}
#Number of rows and columns
dim(application_data)

str(application_data)

summary(application_data)
```

```{r}
# Data overview
skim(application_data)

#Using options() to ensure that numbers are not displayed in scientific notation
options(scipen = 999)

#Inspecting the % of missing values across all columns
cardinalities <- t(as.data.frame(map(application_data, ~mean(is.na(.)))))
colnames(cardinalities) <- "Percentage_missing"
```

## Data cleaning
Data cleaning involves either finding and removing columns or rows with null values or replacing the null value with any suitable value. This is done to make a reliable analysis.
Their deletion depends on whether a given column adds value to our analysis or not.
But at any given point removing columns with more than 30% data seems logical though there is no said rule for it.

```{r}
# Counting the number of variables with more than 40% missing values
application_data <- as.data.frame(application_data)
missing <- colSums(is.na(application_data))

cardinalities <- as.data.frame(missing)
n <- nrow(application_data)
limit <- 0.4 * n

cardinalities$delete <- cardinalities$missing > limit
```

Whether we choose to remove columns with 20 or 48% missing values results in the same number of columns removed (45). 

# Multi-column analysis












