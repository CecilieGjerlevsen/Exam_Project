---
title: "Loan defaults"
date: "12/06/2021"
output: html_document
editor_options: 
  chunk_output_type: inline
---

Loading the three datasets
```{r, echo=FALSE}
#Loading the dataset
application_data <- read.csv("application_data.csv")
descriptions <- read.csv("columns_description.csv")
```

Loading packages:
```{r, echo=FALSE}
pacman::p_load(
  ggplot2, #for plotting graphics
  ggmap, #for plotting maps
  RColorBrewer, #provide pretty colour palettes
  lubridate, #for converting time variables into date format
  hrbrthemes, #for more style themes
  data.table, #for working with data.table
  dplyr, #for count function
  tidytext, #for text mining functions such as unnest()
  wordcloud,
  tm, #for removing stopwords
  ggdist, #for raincloud plots
  tidyquant, #for theme_tq
  prismatic, #for color handling in plots
  gganimate, #for animated plots
  gifski, #for rendering animated plots as GIFs instead of single frames
  tidyverse,
  skimr,
  psych,
  ggcorrplot,
  scales,
  missMDA
)

```

##Data profiling
# Cardinalities (single-column analysis)
```{r}
#Number of rows and columns
dim(application_data)
#str(application_data)

#Inspecting the % of missing values across all columns
#NA_inspection <- t(as.data.frame(map(application_data, ~mean(is.na(.)))))
#colnames(NA_inspection) <- "Percentage_missing"
```

#Data inspection - general summary statistics
```{r, results='hide'}
# Data overview

#Using options() to ensure that numbers are not displayed in scientific notation
options(scipen = 999)

#counting max, min, mean, median of each of the columns
cardinalities <- as.data.table(describe(application_data),keep.rownames=TRUE)

#Getting the datatype/class of each column
cardinalities$class <- sapply(application_data,class)

#Now we want to get the numnber of unique values in each of the columns
cardinalities$distinct <- sapply(application_data, n_distinct)

#Calculating the number of NA in each column, where we first replace all blanks "" with NA
application_data[application_data == ""] <- NA
cardinalities$missing <- colSums(is.na(application_data))
```

## Data cleaning
"Data cleaning involves either finding and removing columns or rows with null values or replacing the null value with any suitable value. This is done to make a reliable analysis.
Their deletion depends on whether a given column adds value to our analysis or not.
But at any given point removing columns with more than 30% data seems logical though there is no said rule for it."

#Aggregating information on documents into one new variable, DOCUMENTS_PROVIDED
```{r}
master_data <- application_data
master_data$DOCUMENTS_PROVIDED <- rowSums(select(master_data,c(97:116)) == "1")

#Then, we remove the binary document variables
master_data <- master_data[,-c(97:116)]
```

#Changing the variables from days to years
```{r}
master_data$age <- floor(abs(master_data$DAYS_BIRTH / 365))
master_data$years_employed <- abs(master_data$DAYS_EMPLOYED / 365)
master_data$years_registration <- abs(master_data$DAYS_REGISTRATION / 365)
master_data$years_publish <- abs(master_data$DAYS_ID_PUBLISH / 365)
master_data$years_phone_change <- abs(master_data$DAYS_LAST_PHONE_CHANGE / 365)

#Removing the old variables
master_data <- master_data[,!(names(master_data) %in% c("DAYS_BIRTH","DAYS_EMPLOYED","DAYS_REGISTRATION","DAYS_ID_PUBLISH","DAYS_LAST_PHONE_CHANGE"))]
```

#Removing columns with NA values over a limit of 40%
```{r}
# Counting the number of variables with more than 40% missing values.
n <- nrow(master_data) 
limit <- 0.4 * n #setting a limit for which columns to remove, where the limit is set to be 40%

cardinalities$delete <- cardinalities$missing > limit #creating a column with TRUE and FALSE values depending on whether the column should be deleted or not

sum(cardinalities$delete) #counting the number of columns to be deleted

master_data <- master_data[, which(colMeans(is.na(master_data)) < 0.4)] #Overwriting the dataset where the 49 columns are deleted. 
```

## Plot of percentage missing values
```{r}
cardinalities$pct_missing <- cardinalities$missing/n * 100

#options(repr.plot.width = 30, repr.plot.height = 0.75)

cardinalities$rn <- factor(cardinalities$rn, levels = cardinalities$rn)

cardinalities %>% 
  ggplot(aes(rn,pct_missing)) +
  geom_segment( aes(x=rn, xend=rn, y=0, yend=pct_missing)) +
  geom_point(color = "blue") +
  geom_hline(yintercept=40, linetype="dashed", color = "red") +
  labs(x = "", y = "Percentage missing",title = "Percentage missing values in each column") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(limits=cardinalities$rn)
```


Whether we choose to remove columns with 20 or 48% missing values results in the same number of columns removed (45). When delaing with many missing values, you can choose between different alternatives: Remove the columns, remove the entire row (in this case loan applicant) or insert some sort of proxy (i.e. the mean or median of the column). In this case we consider the best option to be to delete the columns, since we deem this to be the option that will result in the least bias. Furthermore going through the columns to delete, they are not deemed as essential features for our analysis. 


#Correlation analysis on the remaining 58 variables is conducted to remove unnecessary columns

Trial and error start for "correct" plot with Gender variable
```{r}
#We want to plot if the nominal scaled variables have any correlation witht the TARGET variable to remove unnecessary variables. 
ggplot(master_data, aes(x = TARGET, fill = CODE_GENDER)) + 
  geom_bar(position = "dodge") + 
  labs(y = "Count", x = "Target variable", fill = "Gender") + 
  scale_x_continuous(breaks=c(0,1)) + 
  scale_fill_discrete(labels = c("Female", "Male", "Unknown"))
```
Found the correct way to plot and will now go through all possible feature variables which are categorical or binary:

Starting with the variables: Did the client provide XX? Column 22 to 27 in Master file.
- Did client provide mobile phone (1=YES, 0=NO) = TO DROP
- Did client provide work phone (1=YES, 0=NO) = TO KEEP
- Did client provide home phone (1=YES, 0=NO) = TO DROP (two variables which indicate home phone, we remove this one because there is a discrepancy between name of variable and description of variable and multicollinarity)
- Was mobile phone reachable (1=YES, 0=NO) = TO DROP
- Did client provide home phone (1=YES, 0=NO) = TO KEEP
- Did client provide email (1=YES, 0=NO) = TO KEEP

```{r}
for(i in 1:6) 
 print(ggplot(master_data, aes(x = TARGET, fill = factor(master_data[,c(17+i)]))) +
 geom_bar(position = "dodge") +
 labs(y = "Count", x = "Target variable", fill = "Did client provide XX?") + 
 scale_x_continuous(breaks=c(0,1)) + 
 scale_fill_discrete(labels = c("No", "Yes")))
```

NOW WE REMOVE THE VARIABLES "FLAG_MOBIL" "FLAG_WORK_PHONE" and "FLAG_CONT_MOBILE"
```{r}
master_data <- master_data[,!(names(master_data) %in% c("FLAG_MOBIL", "FLAG_WORK_PHONE", "FLAG_CONT_MOBILE"))]
```

Now we check the variables FLAG_OWN_CAR and FLAG_OWN_REALTY, TO KEEP BOTH
```{r}
for(i in 1:2) 
 print(ggplot(master_data, aes(x = TARGET, fill = factor(master_data[,c(4+i)]))) +
 geom_bar(position = "dodge") +
 labs(y = "Count", x = "Target variable", fill = "Do client own XX?") + 
 scale_x_continuous(breaks=c(0,1)) + 
 scale_fill_discrete(labels = c("No", "Yes")))
```


We check the variables regarding if addresses match some other info:
```{r}
for(i in 1:6) 
 print(ggplot(master_data, aes(x = TARGET, fill = factor(master_data[,c(26+i)]))) +
 geom_bar(position = "dodge") +
 labs(y = "Count", x = "Target variable", fill = "Does XX match YY?") +
 scale_x_continuous(breaks=c(0,1)) + 
 scale_fill_discrete(labels = c("Same","Different")))
```

```{r}
model.matrix(~0+., master_data[,c("REG_REGION_NOT_LIVE_REGION","REG_REGION_NOT_WORK_REGION","LIVE_REGION_NOT_WORK_REGION","REG_CITY_NOT_LIVE_CITY","REG_CITY_NOT_WORK_CITY","LIVE_CITY_NOT_WORK_CITY","TARGET")]) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)
```

We remove the 3 variables which are related to region, since if it matches at the city level it is expected to also match at region level.
```{r}
master_data <- master_data[,!(names(master_data) %in% c("REG_REGION_NOT_LIVE_REGION","REG_REGION_NOT_WORK_REGION","LIVE_REGION_NOT_WORK_REGION"))]
```

#WE MAKE CORRELATIONS PLOT TO FIND UNECCESSARY VARIABLES
```{r}
data_cor <- as.data.frame(cor(master_data[ ,sapply(master_data,is.numeric)], use = "complete.obs")[,2])
names(data_cor)[1] <- "correlation"

ggcorrplot(data_cor, lab = TRUE)

```
Making a new cardinalities for the dataset with final columns:
```{r}
#counting max, min, mean, median of each of the columns
cardinalities_master <- as.data.table(describe(master_data),keep.rownames=TRUE)

#Getting the datatype/class of each column
cardinalities_master$class <- sapply(master_data,class)

#Now we want to get the numnber of unique values in each of the columns
cardinalities_master$distinct <- sapply(master_data, n_distinct)
```

We now inspect the 'Cardinalities' datatable to see if any irregularities pop up. We notice:
CODE_gender (3 distinct), CNT_children (max of 19), AMT_INCOME_TOAL and AMT_credit (only 2548 and 5603 distinct values)

when inspecting the columns which are denominated in a currency, we thought the values were higher than expected, since we thought it was in american $, however we are now under the impression that it is in Indian rupees. 

The variables called FLAG_Document_# (from 2-21) all have different means indicating that there is a difference in how many documents each applicant have handed in. However, we do not have any information about the content of each document. To use the information in these variables in more manageable way, we aggregate them to one new variables, which counts the number of documents each applicant has submitted.

We start out by checking the binary variables to see if they all have 2 distinct values. We observe that 'CODE_Gender' has 3 distinct values, where one of them is 'XNA'. We check how many of the observations have this value. 

```{r}
unique(master_data$CODE_GENDER) #view the distinct values
sum(master_data$CODE_GENDER == "XNA")
sum(master_data$CODE_GENDER == "M") / n
#counting the number of distinct values of CODE_gender, the number is 4, and the most frequent gender is female.

#We replace XNA with the female as this is the most common gender, 65% female
master_data$CODE_GENDER <- replace(master_data$CODE_GENDER,master_data$CODE_GENDER == "XNA","M")
```


#IMPUTATION
Generalized Imputation: In this case, we calculate the mean or median for all non missing values of that variable then replace missing value with mean or median.

Impute missing values in application_data:
For categorical variables with low null percentage, impute with the most frequent items.
For categorical variables with high null percentage, create new category as not to influence the analysis.

```{r}
#creating a function that "calculates" the most frequent observation of a categorical value
imputed_data <- master_data
calculate_mode <- function(x) {
  uniqx <- unique(na.omit(x))
  uniqx[which.max(tabulate(match(x, uniqx)))]
}
#testing it for the gender variable:
calculate_mode(master_data$CODE_GENDER)

#looking at cardinalities_master and finding which categorical values have missing values and imputing the most frequent:

#NAME_TYPE_SUITE
imputed_data["NAME_TYPE_SUITE"] <- replace(imputed_data["NAME_TYPE_SUITE"],is.na(imputed_data["NAME_TYPE_SUITE"]), calculate_mode(imputed_data$NAME_TYPE_SUITE))

#Occupation type is next, but given a lot of missing values, we create new variable called "Unknown" instead of imputed most frequent
imputed_data["OCCUPATION_TYPE"] <- replace(imputed_data["OCCUPATION_TYPE"],is.na(imputed_data["OCCUPATION_TYPE"]), "Unknown")
```

We now have to go through the numerical values which have missing values:
```{r}
for(i in 1:ncol(imputed_data))
  imputed_data[ , i][is.na(imputed_data[ , i])] <- mean(imputed_data[ , i], na.rm=TRUE)
```
Making a new cardinalities for the dataset with no missing values:
```{r}
#counting max, min, mean, median of each of the columns
cardinalities_imputed <- as.data.table(describe(imputed_data),keep.rownames=TRUE)

#Getting the datatype/class of each column
cardinalities_imputed$class <- sapply(imputed_data,class)

#Now we want to get the numnber of unique values in each of the columns
cardinalities_imputed$distinct <- sapply(imputed_data, n_distinct)
```


##Outliers
We look at the categorical values, the number of distinct answers and the frequency:

```{r}
count(imputed_data, NAME_EDUCATION_TYPE)
#all variables which start with 'NAME_' look okay, except for education where there is both 'academic' and 'higher education' which might be the same. 

#WEEKDAY_APPR_PROCESS_START okay. 
count(imputed_data, WEEKDAY_APPR_PROCESS_START)

#WEEKDAY_APPR_PROCESS_START okay. Might make 'business entity into 1 category'
count(imputed_data,ORGANIZATION_TYPE)
```

We now look at min and max to see if anything doesn't match our expectations.
- Income variable: AMT_INCOME_TOTAL
- 


We now look at the income variable:
```{r}
ggplot(imputed_data,aes(AMT_INCOME_TOTAL)) + 
  geom_dots()
```

Imputing the extreme value above 30.000.000 in income with the mean:
```{r}
sum(imputed_data$AMT_INCOME_TOTAL > 30000000)
imputed_data$AMT_INCOME_TOTAL <- replace(imputed_data$AMT_INCOME_TOTAL,imputed_data$AMT_INCOME_TOTAL > 30000000, mean(imputed_data$AMT_INCOME_TOTAL))
```




















